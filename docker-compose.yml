# version: "3"
services:
  traefik:
    image: traefik:v2.3.7
    command: 
      - --providers.docker
      - --providers.docker.exposedByDefault=false
      - --entrypoints.web.address=:80
    ports:
      - 80:80
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  default:
    image: traefik/whoami
    environment:
      - WHOAMI_NAME=My Default Server
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.default.rule=Path(`/{path:.*}`)"

  server1:
    image: traefik/whoami
    environment:
      - WHOAMI_NAME=My First Test Server
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.server1.rule=Path(`/server1{path:.*}`)"

  server2:
    image: traefik/whoami
    environment:
      - WHOAMI_NAME=My Second Test Server
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.server2.rule=Path(`/server2{path:.*}`)"

  immich-machine-learning:
    # env_file: .env
    container_name: immich_machine_learning
    #? For hardware acceleration, add one of -[armnn, cuda, openvino] to the image tag.
    #* Example tag: ${IMMICH_VERSION:-release}-cuda
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}
    # extends: #? uncomment this section for hardware acceleration - see https://immich.app/docs/features/ml-hardware-acceleration
    #*   file: hwaccel.ml.yml
    #*   service: cpu #? set to one of [armnn, cuda, openvino, openvino-wsl] for accelerated inference - use the `-wsl` version for WSL2 where applicable
    volumes:
      - model-cache:/cache
    labels:
      # - "com.centurylinklabs.watchtower.enable=false"
      # - "com.centurylinklabs.watchtower.monitor-only=true"
      # - "com.centurylinklabs.watchtower.depends-on=immich_server"
      - "traefik.enable=true"
      - "traefik.http.routers.immich_machine_learning.rule=Path(`/immich_ml_remote{path:.*}`)"
    restart: always
    # ports: #? host port: container port
      # - 80:3003
    expose:
      - 3003

volumes:
  model-cache:
