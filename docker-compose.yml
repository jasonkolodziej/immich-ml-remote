# version: "3"
services:
  # traefik:
  #   image: traefik:v2.3.7
  #   command:
  #     - --providers.docker
  #     - --providers.docker.exposedByDefault=false
  #     - --entrypoints.web.address=:80
  #   ports:
  #     - 80:80
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock

  # default:
  #   image: traefik/whoami
  #   environment:
  #     - WHOAMI_NAME=My Default Server
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.default.rule=Path(`/{path:.*}`)"

  # server1:
  #   image: traefik/whoami
  #   environment:
  #     - WHOAMI_NAME=My First Test Server
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.server1.rule=Path(`/server1{path:.*}`)"

  # server2:
  #   image: traefik/whoami
  #   environment:
  #     - WHOAMI_NAME=My Second Test Server
  #   labels:
  #     - "traefik.enable=true"
  #     - "traefik.http.routers.server2.rule=Path(`/server2{path:.*}`)"

  immich-machine-learning:
    # env_file: .env
    container_name: immich_machine_learning
    #* Example tag: ${IMMICH_VERSION:-release}-cuda
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION}${HW_ACCELERATOR} #? For hardware acceleration, add oneOf( -armnn, -cuda, -openvino, -openvino-wsl) to the image tag.
    volumes:
      - model-cache:/cache
    labels:
      # - "com.centurylinklabs.watchtower.enable=false"
      # - "com.centurylinklabs.watchtower.monitor-only=true"
      # - "com.centurylinklabs.watchtower.depends-on=immich_server"
      - "traefik.enable=true"
      - "traefik.http.routers.immich_machine_learning.rule=Path(`/immich_ml_remote{path:.*}`)"
    # restart: always
    # ports: #? host port: container port
    #   - 3003:3003
    expose:
      - 3003
    # extends: #? uncomment this section for hardware acceleration
    #*   file: hwaccel.ml.yml #? - see: https://immich.app/docs/features/ml-hardware-acceleration
    #*   service: cpu #? set to oneOf( -armnn, -cuda, -openvino, -openvino-wsl) for accelerated inference -- use the `-wsl` version for WSL2 where applicable
    #
    #? For GCP CloudRun NVIDIA: https://cloud.google.com/run/docs/container-contract#nvidia-libraries
    #    By default, all of the NVIDIA L4 driver libraries are mounted under /usr/local/nvidia/lib64.
    #      If your service is unable to find the provided libraries,
    #      update the search path for the dynamic linker by adding the line:
    #      `ENV LD_LIBRARY_PATH /usr/local/nvidia/lib64:${LD_LIBRARY_PATH}`
    #      to your Dockerfile.
    #!   Note: that you can also set `LD_LIBRARY_PATH` as an environment variable for the Cloud Run service,
    #!     if you have an existing image and don't want to rebuild the image with an updated `Dockerfile`.
    #
    #*   - If you want to use a `CUDA` version greater than `12.2`, the easiest way is to depend on a
    #*      newer NVIDIA base image with forward compatibility packages already installed.
    #*   - Another option is to manually install the NVIDIA forward compatibility packages and add them to LD_LIBRARY_PATH.
    #*      Consult NVIDIA's compatibility matrix to determine which CUDA versions are forward compatible with the provided
    #*      NVIDIA driver version (535.129.03).

    #? For `image: immich-machine-learning:${IMMICH_VERSION:-release}-cuda` see: https://immich.app/docs/features/ml-hardware-acceleration#cuda
    #? Requirements:
    #*  - The GPU must have compute capability 5.2 or greater.
    #*  - The server must have the official NVIDIA driver installed.
    #*  - The installed driver must be >= 535 (it must support CUDA 12.2).
    #! Without using `extends:`
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
                - gpu

volumes:
  model-cache:
